# Data Pipelines with Apache Airflow

## Purpose

The purpose of this project is to automate and monitor a cloud data warehouse ETL pipeline for a fictitious music-streaming startup called Sparkify that is interested in understanding which songs users are listening to. Sparkify has song and event-log datasets stored on S3, at `s3://udacity-dend/song_data` and `s3://udacity-dend/log_data` respectively. This project uses Apache Airflow to automate, quality-check, and monitor an ETL pipeline that copies each dataset from S3 to staging tables in Amazon Redshift then inserts data into star schema tables from the staging tables.

## Datasets

### Song Dataset

The files found in `s3://udacity-dend/song_data` are a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are paths to two files in this dataset:

    s3://udacity-dend/song_data/A/B/C/TRABCEI128F424C983.json
    s3://udacity-dend/song_data/A/A/B/TRAABJL12903CDCF1A.json
    
Below is an example of what a single song file, `TRAABJL12903CDCF1A.json`, looks like:

    {"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}

### Log Dataset

The second dataset found in `s3://udacity-dend/log_data` consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

The log files in this dataset are partitioned by year and month. For example, here are filepaths to two files in this dataset:

    s3://udacity-dend/log_data/2018/11/2018-11-12-events.json
    s3://udacity-dend/log_data/2018/11/2018-11-13-events.json

Below is an example of what the data in a log file, `2018-11-12-events.json`, looks like:

![event log](https://video.udacity-data.com/topher/2019/February/5c6c15e9_log-data/log-data.png)

## Redshift Data Warehouse Design

The staging tables' columns directly correspond to those of the files in the datasets. The star schema is modeled as follows:

###### Fact Table
1. **songplays** - records in log data associated with song plays i.e. records with page `NextSong`
    - *songplay_id*, *start_time*, *user_id*, *level*, *song_id*, *artist_id*, *session_id*, *location*, *user_agent*
    
###### Dimension Tables
2. **users** - users in the app
    - *user_id*, *first_name*, *ast_name*, *gender*, *level*
    
3. **songs** - songs in music database
    - *song_id*, *title*, *artist_id*, *year*, *duration*
    
4. **artists** - artists in music database
    - *artist_id*, *name*, *location*, *latitude*, *longitude*
    
5. **time** - timestamps of records in **songplays** broken down into specific units
    - *start_time*, *hour*, *day*, *week*, *month*, *year*, *weekday*  

For more details on how this data is modeled as it pertains to analytical queries, see [this project](https://github.com/spaydar/Data-Modeling-with-Postgres).

## Airflow Pipeline

###### DAG

The data pipeline uses a single DAG, which implements the following requirements in its `default parameters`:

- The DAG does not have dependencies on past runs
- On failure, a task is retried 3 times
- Task retries happen every 5 minutes
- Catchup (schedule backfill) is turned off
- Emails are not sent on task retry

###### Tasks

Tasks dependencies are configured in order to adhere to the following graph:

![task dependency graph view](https://video.udacity-data.com/topher/2019/January/5c48ba31_example-dag/example-dag.png)

###### Operators

The data pipeline uses 4 modular custom operators that stage the data, transform the data, and run checks on data quality. Each operator runs SQL statements against the Redshift database, which they connect to using Airflow's `PostgresHook`.

*Stage Operator*
The stage operator can load any JSON formatted files from S3 to Redshift. The operator creates and runs a SQL `COPY` statement based on the parameters provided. The operator's parameters specify where in S3 the file is loaded from and what is the targeted table. Here are examples of `COPY` statements generated and executed by the stage operator:
    
    COPY staging_events FROM 's3://udacity-dend/log_data' ACCESS_KEY_ID '{aws_key}' SECRET_ACCESS_KEY '{aws_secret}' JSON 's3://udacity-dend/log_json_path.json';
    
    COPY staging_songs FROM 's3://udacity-dend/song_data' ACCESS_KEY_ID '{aws_key}' SECRET_ACCESS_KEY '{aws_secret}' JSON 'auto';

*Fact and Dimension Operators*


*Data Quality Operator*

